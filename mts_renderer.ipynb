{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scalar_rgb', 'scalar_spectral', 'cuda_ad_rgb', 'llvm_ad_rgb']\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import sys \n",
    "import numpy as np\n",
    "\n",
    "import mitsuba as mi \n",
    "print(mi.variants())\n",
    "mi.set_variant('cuda_ad_rgb')\n",
    "\n",
    "import drjit as dr\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise Exception('No GPU available, please use a GPU to train your neural network.')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objects:  6778\n",
      "envmaps:  8\n"
     ]
    }
   ],
   "source": [
    "objects = os.listdir('data/03001627')\n",
    "objects = ['data/03001627/' + o + \"/models/model_normalized.obj\" for o in objects]\n",
    "print(\"objects: \", len(objects))\n",
    "\n",
    "envmaps = os.listdir('data/envmaps')\n",
    "envmaps = ['data/envmaps/' + e for e in envmaps]\n",
    "print(\"envmaps: \", len(envmaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "camPosAll = [[1,1,1],[1,-1,-1],[-1,1,-1],[-1,-1,1],[1,1,-1],[1,-1,1],[-1,1,1],[-1,-1,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 0.7\n",
    "SIZE = 128\n",
    "NVIEW = 8\n",
    "# N = 100\n",
    "N = 4000\n",
    "\n",
    "def load_sensor_random():\n",
    "    look_at = mi.ScalarTransform4f.look_at(\n",
    "        origin=[2, 1, 0],\n",
    "        target=[0, 0, 0],\n",
    "        up=[0, 1, 0]\n",
    "    )\n",
    "    scale = mi.ScalarTransform4f.scale([SCALE, SCALE, SCALE])\n",
    "    rotation = mi.ScalarTransform4f.rotate([0, 1, 0], 360 * random.random())\n",
    "    trans = scale @ rotation @ look_at\n",
    "\n",
    "    return mi.load_dict({\n",
    "        'type' : 'orthographic',\n",
    "        'film' : {\n",
    "            'type' : 'hdrfilm',\n",
    "            'width' : SIZE,\n",
    "            'height' : SIZE,\n",
    "            'banner' : False\n",
    "        },\n",
    "        'sampler' : {\n",
    "            'type' : 'multijitter'\n",
    "        },\n",
    "        'to_world' : trans\n",
    "    })\n",
    "\n",
    "def load_sensor_fixed(id):\n",
    "    look_at = mi.ScalarTransform4f.look_at(\n",
    "        origin=camPosAll[id],\n",
    "        target=[0, 0, 0],\n",
    "        up=[0, 1, 0]\n",
    "    )\n",
    "    scale = mi.ScalarTransform4f.scale([SCALE, SCALE, SCALE])\n",
    "    #rotation = mi.ScalarTransform4f.rotate([0, 1, 0], 360 * id / NVIEW )\n",
    "    #trans = scale @ rotation @ look_at\n",
    "    trans = scale @ look_at\n",
    "\n",
    "    return mi.load_dict({\n",
    "        'type' : 'orthographic',\n",
    "        'film' : {\n",
    "            'type' : 'hdrfilm',\n",
    "            'width' : SIZE,\n",
    "            'height' : SIZE,\n",
    "            'banner' : False\n",
    "        },\n",
    "        'sampler' : {\n",
    "            'type' : 'multijitter'\n",
    "        },\n",
    "        'to_world' : trans\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7618e2dab549308f51f3c79b9af318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n",
      "​[OBJMesh] Error while loading OBJ file \"model_normalized.obj\": mesh contains invalid vertex normal data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m     34\u001b[0m     scene \u001b[38;5;241m=\u001b[39m mi\u001b[38;5;241m.\u001b[39mload_dict(scene)\n\u001b[1;32m---> 35\u001b[0m     views \u001b[38;5;241m=\u001b[39m [mi\u001b[38;5;241m.\u001b[39mrender(scene, spp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, sensor\u001b[38;5;241m=\u001b[39mload_sensor_fixed(\u001b[38;5;28mid\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NVIEW)]\n\u001b[0;32m     36\u001b[0m     views \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnan_to_num((image \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.2\u001b[39m))\u001b[38;5;241m.\u001b[39mtorch()\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m views]\n\u001b[0;32m     37\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(views, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m     34\u001b[0m     scene \u001b[38;5;241m=\u001b[39m mi\u001b[38;5;241m.\u001b[39mload_dict(scene)\n\u001b[1;32m---> 35\u001b[0m     views \u001b[38;5;241m=\u001b[39m [\u001b[43mmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_sensor_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NVIEW)]\n\u001b[0;32m     36\u001b[0m     views \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnan_to_num((image \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.2\u001b[39m))\u001b[38;5;241m.\u001b[39mtorch()\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m views]\n\u001b[0;32m     37\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(views, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32mc:\\Users\\beltegeuse\\projects\\pytorch-3d-point-cloud-generation\\.venv\\lib\\site-packages\\mitsuba\\python\\util.py:522\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(scene, params, sensor, integrator, seed, seed_grad, spp, spp_grad)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m seed_grad \u001b[38;5;241m==\u001b[39m seed:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe primal and differential seed should be different \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    520\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto ensure unbiased gradient computation!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_RenderOp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_grad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mspp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspp_grad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\beltegeuse\\projects\\pytorch-3d-point-cloud-generation\\.venv\\lib\\site-packages\\drjit\\router.py:5776\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5774\u001b[0m     output \u001b[38;5;241m=\u001b[39m inst\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;241m*\u001b[39m_dr\u001b[38;5;241m.\u001b[39mdetach(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m   5775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5776\u001b[0m     output \u001b[38;5;241m=\u001b[39m inst\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{ k: _dr\u001b[38;5;241m.\u001b[39mdetach(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() })\n\u001b[0;32m   5777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _dr\u001b[38;5;241m.\u001b[39mgrad_enabled(output):\n\u001b[0;32m   5778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrjit.custom(): the return value of CustomOp.eval() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5779\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould not be attached to the AD graph!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\beltegeuse\\projects\\pytorch-3d-point-cloud-generation\\.venv\\lib\\site-packages\\mitsuba\\python\\util.py:377\u001b[0m, in \u001b[0;36m_RenderOp.eval\u001b[1;34m(self, scene, sensor, params, integrator, seed, spp)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspp \u001b[38;5;241m=\u001b[39m spp\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dr\u001b[38;5;241m.\u001b[39msuspend_grad():\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscene\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43msensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevelop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "images = []\n",
    "pbar = tqdm(total=N, file=sys.stdout)\n",
    "for i in range(N):\n",
    "    path = objects[i]\n",
    "    env = random.choice(envmaps)\n",
    "    scene = {\n",
    "        'type' : 'scene',\n",
    "        'integrator' : {\n",
    "            'type' : 'path'\n",
    "        },\n",
    "        'emitter' : {\n",
    "            'type': 'envmap',\n",
    "            'filename': env,\n",
    "            'to_world': mi.ScalarTransform4f.rotate([0, 1, 0], 360 * random.random())\n",
    "        },\n",
    "        'shape' : {\n",
    "            'type' : 'obj',\n",
    "            'filename' : path,\n",
    "            'bsdf' : {\n",
    "                'type': 'twosided',\n",
    "                'material' : {\n",
    "                'type' : 'diffuse',\n",
    "                    'reflectance' : {\n",
    "                        'type' : 'rgb',\n",
    "                        'value' : [0.8, 0.8, 0.8]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try: \n",
    "        scene = mi.load_dict(scene)\n",
    "        views = [mi.render(scene, spp=64, sensor=load_sensor_fixed(id)) for id in range(NVIEW)]\n",
    "        views = [torch.nan_to_num((image ** (1.0 / 2.2)).torch().clamp(0.0, 1.0)) for image in views]\n",
    "        image = torch.concat(views, dim=-1).permute(2, 0, 1).cpu()\n",
    "        images.append(image)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Generated {path} images\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pbar.update(1)\n",
    "        dr.flush_kernel_cache()\n",
    "        dr.flush_malloc_cache()\n",
    "        continue\n",
    "\n",
    "# Show only\n",
    "# fig, axs = plt.subplots(1, len(images), figsize=(len(images) * 5, 5))\n",
    "# for ax, image in zip(axs, images):\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "random.seed(SEED)\n",
    "pbar = tqdm(total=N, file=sys.stdout)\n",
    "for i in range(N):\n",
    "    path = objects[i]\n",
    "    scene = {\n",
    "        'type' : 'scene',\n",
    "        'integrator' : {\n",
    "            'type' : 'depth'\n",
    "        },\n",
    "        'shape' : {\n",
    "            'type' : 'obj',\n",
    "            'filename' : path,\n",
    "            'bsdf' : {\n",
    "                'type': 'twosided',\n",
    "                'material' : {\n",
    "                'type' : 'diffuse',\n",
    "                    'reflectance' : {\n",
    "                        'type' : 'rgb',\n",
    "                        'value' : [0.8, 0.8, 0.8]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try: \n",
    "        scene = mi.load_dict(scene)\n",
    "        views = [mi.render(scene, spp=16, sensor=load_sensor_fixed(id)) for id in range(NVIEW)]\n",
    "        views = [torch.nan_to_num((image[:,:,0] / 3.0).torch()) for image in views]\n",
    "        image = torch.stack(views, dim=-1).permute(2, 0, 1).cpu()\n",
    "        depths.append(image)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Generated {path} images\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "# Show only\n",
    "# fig, axs = plt.subplots(1, len(images), figsize=(len(images) * 5, 5))\n",
    "# for ax, image in zip(axs, images):\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW = 4\n",
    "fig, axs = plt.subplots(2, SHOW, figsize=(SHOW * 5, 2 * 5))\n",
    "showid = [random.randint(0, len(images) - 1) for _ in range(SHOW)]\n",
    "viewid = [random.randint(0, NVIEW - 1) for _ in range(SHOW)]\n",
    "for i in range(SHOW * 2):\n",
    "    ax = axs[i // SHOW, i % SHOW]\n",
    "    ax.axis(\"off\")\n",
    "    if i < SHOW:\n",
    "        id = showid[i]\n",
    "        idview = viewid[i]\n",
    "        ax.imshow(images[id][idview * 3: (idview + 1)*3].cpu().permute(1, 2, 0))\n",
    "    else:\n",
    "        id = showid[i - SHOW]\n",
    "        idview = viewid[i - SHOW]\n",
    "        ax.imshow(depths[id][idview].cpu(), cmap='gray')\n",
    "plt.show()\n",
    "print(depths[0][0])\n",
    "print(depths[0][0].mean())\n",
    "mask = depths[0][0] > 0.0\n",
    "print(depths[0][0][mask].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, NVIEW, figsize=(15, 5))\n",
    "id = random.randint(0, len(images) - 1)\n",
    "for i in range(NVIEW):\n",
    "    axs[0, i].imshow(images[id][i * 3: (i + 1) * 3].cpu().permute(1, 2, 0))\n",
    "for i in range(NVIEW):\n",
    "    axs[1, i].imshow(depths[id][i].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def deconv2d_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def linear_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_c, out_c),\n",
    "        nn.BatchNorm1d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def pixel_bias(outViewN, outW, outH, renderDepth):\n",
    "    X, Y = torch.meshgrid([torch.arange(outH), torch.arange(outW)])\n",
    "    X, Y = X.float(), Y.float() # [H,W]\n",
    "    X = (X / (outH - 1) - 0.5) * 2\n",
    "    Y = (Y / (outW - 1) - 0.5) * 2\n",
    "    initTile = torch.cat([\n",
    "        X.repeat([outViewN, 1, 1]), # [V,H,W]\n",
    "        Y.repeat([outViewN, 1, 1]), # [V,H,W]\n",
    "        torch.ones([outViewN, outH, outW]).float() * renderDepth, \n",
    "        torch.zeros([outViewN, outH, outW]).float(),\n",
    "    ], dim=0) # [4V,H,W]\n",
    "\n",
    "    return initTile.unsqueeze_(dim=0) # [1,4V,H,W]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder of Structure Generator\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = conv2d_block(3, 96)\n",
    "        self.conv2 = conv2d_block(96, 128)\n",
    "        self.conv3 = conv2d_block(128, 192)\n",
    "        self.conv4 = conv2d_block(192, 256)\n",
    "        self.conv5 = conv2d_block(256, 256) # New to make the size 4x4\n",
    "        self.fc1 = linear_block(4096, 2048) # After flatten\n",
    "        self.fc2 = linear_block(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Build Decoder\"\"\"\n",
    "    def __init__(self, outViewN, outW, outH, renderDepth):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.outViewN = outViewN\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = linear_block(512, 1024)\n",
    "        self.fc2 = linear_block(1024, 2048)\n",
    "        self.fc3 = linear_block(2048, 4096)\n",
    "        self.deconv1 = deconv2d_block(256, 192)\n",
    "        self.deconv2 = deconv2d_block(192, 128)\n",
    "        self.deconv3 = deconv2d_block(128, 96)\n",
    "        self.deconv4 = deconv2d_block(96, 64)\n",
    "        self.deconv5 = deconv2d_block(64, 48)\n",
    "        self.pixel_conv = nn.Conv2d(48, outViewN*4, 1, stride=1, bias=False)\n",
    "        self.pixel_bias = pixel_bias(outViewN, outW, outH, renderDepth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x.view([-1, 256, 4, 4])\n",
    "        x = self.deconv1(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv2(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv3(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv4(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv5(F.interpolate(x, scale_factor=2))\n",
    "        x = self.pixel_conv(x)\n",
    "        # x[:, :self.outViewN*2] = torch.tanh(x[:, :self.outViewN*2]) # XYZ\n",
    "        # x[:, self.outViewN*2:self.outViewN*3] = F.sigmoid(x[:, self.outViewN*2:self.outViewN*3])\n",
    "        # x[:, self.outViewN*3:] = torch.sigmoid(x[:, self.outViewN*3:]) # maskLogit\n",
    "        x = x + self.pixel_bias.to(x.device)\n",
    "        XYZ, maskLogit = torch.split(\n",
    "            x, [self.outViewN * 3, self.outViewN], dim=1)\n",
    "        return XYZ, maskLogit\n",
    "\n",
    "\n",
    "class Structure_Generator(nn.Module):\n",
    "    \"\"\"Structure generator components in PCG\"\"\"\n",
    "\n",
    "    def __init__(self, encoder=None, decoder=None,\n",
    "                 outViewN=8, outW=128, outH=128, renderDepth=1.0):\n",
    "        super(Structure_Generator, self).__init__()\n",
    "\n",
    "        if encoder: self.encoder = encoder\n",
    "        else: self.encoder = Encoder()\n",
    "\n",
    "        if decoder: self.decoder = decoder\n",
    "        else: self.decoder = Decoder(outViewN, outW, outH, renderDepth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        XYZ, maskLogit = self.decoder(latent)\n",
    "        \n",
    "        return XYZ, maskLogit\n",
    "\n",
    "# Define the structure generator\n",
    "structure_generator = Structure_Generator(None, None, NVIEW, SIZE, SIZE, 0.5)\n",
    "\n",
    "structure_generator = structure_generator.to(device)\n",
    "# print(structure_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definition\n",
    "l1_loss = nn.L1Loss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "DEPTH_LAMBDA = 1.0\n",
    "BATCHSIZE = 16\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ------ define ground truth------\n",
    "XGT, YGT = torch.meshgrid([\n",
    "    torch.arange(SIZE), # [H,W]\n",
    "    torch.arange(SIZE)]) # [H,W]\n",
    "XGT, YGT = XGT.float(), YGT.float()\n",
    "XGT = (XGT / (SIZE - 1) - 0.5) * 2\n",
    "YGT = (YGT / (SIZE - 1) - 0.5) * 2\n",
    "XYGT = torch.cat([\n",
    "    XGT.repeat([NVIEW, 1, 1]), \n",
    "    YGT.repeat([NVIEW, 1, 1])], dim=0) #[2V,H,W]\n",
    "XYGT = XYGT.to(device) # [1,2V,H,W]\n",
    "XYGT = torch.stack([XYGT for _ in range(BATCHSIZE)], dim=0) # [B,2V,H,W]\n",
    "print(XGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "random.seed(SEED)\n",
    "def random_sample(images, depths):\n",
    "    id = random.randint(0, len(images)-1)\n",
    "    viewid = random.randint(0, NVIEW-1)\n",
    "    image = images[id][viewid * 3: (viewid + 1) * 3, :, :]\n",
    "    depth = depths[id]\n",
    "    return image, depth\n",
    "\n",
    "jobs = []\n",
    "for i in range(len(images)):\n",
    "    for j in range(NVIEW):\n",
    "        jobs.append((i, j))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(structure_generator.parameters(), lr=1e-5)\n",
    "structure_generator.train()\n",
    "\n",
    "EPOCHS = 100\n",
    "for e in range(EPOCHS):\n",
    "    random.shuffle(jobs)\n",
    "    avg_loss = 0\n",
    "    N_STEPS = len(jobs) // BATCHSIZE\n",
    "    pbar = tqdm(total=N_STEPS, file=sys.stdout)\n",
    "    for i in range(0, len(jobs), BATCHSIZE):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_images = []\n",
    "        batch_depths = []\n",
    "        batch_masks = []\n",
    "        for k in range(BATCHSIZE):\n",
    "            d = (i + k) % len(jobs)\n",
    "            image = images[jobs[d][0]][jobs[d][1] * 3: (jobs[d][1] + 1) * 3, :, :]\n",
    "            depth = depths[jobs[d][0]]\n",
    "            batch_images.append(image)\n",
    "            batch_depths.append(depth)\n",
    "            batch_masks.append((depth > 0).to(torch.float))\n",
    "        \n",
    "        batch_images = torch.stack(batch_images, dim=0).to(device)\n",
    "        batch_depths = torch.stack(batch_depths, dim=0).to(device)\n",
    "        batch_masks = torch.stack(batch_masks, dim=0).to(device)\n",
    "        # Run model\n",
    "        # print(batch_images.shape)\n",
    "        XYZ, maskLogit = structure_generator(batch_images)\n",
    "        # print(XYZ.shape)\n",
    "        # print(maskLogit.shape)\n",
    "        # print(maskLogit)\n",
    "        # print(batch_masks)\n",
    "\n",
    "        # Extract data\n",
    "        XY = XYZ[:, :NVIEW * 2, :, :]\n",
    "        depth = XYZ[:, NVIEW * 2:NVIEW * 3, :,  :]\n",
    "        mask = (maskLogit > 0).to(torch.bool)\n",
    "\n",
    "        loss_XYZ = l1_loss(XY, XYGT)\n",
    "        loss_XYZ += l1_loss(depth.masked_select(mask),\n",
    "                            batch_depths.masked_select(mask))\n",
    "        loss_mask = bce_loss(maskLogit, batch_masks)\n",
    "        loss = loss_mask + DEPTH_LAMBDA * loss_XYZ\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        pbar.set_description('step: %d / %d | loss: %f' % (e, EPOCHS, avg_loss / (i // BATCHSIZE + 1)))\n",
    "        pbar.update(1)\n",
    "    losses.append(avg_loss / N_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_generator.eval()\n",
    "id = random.randint(0, len(images)-1)\n",
    "viewid = random.randint(0, NVIEW-1)\n",
    "image = images[id][viewid * 3: (viewid + 1) * 3, :, :]\n",
    "depth = depths[id]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "pred_XYZ, pred_maskLogit = structure_generator(image)\n",
    "pred_XYZ = pred_XYZ[0]\n",
    "maskLogit = pred_maskLogit[0]\n",
    "\n",
    "pred_XY = pred_XYZ[:NVIEW * 2, :, :]\n",
    "pred_depth = pred_XYZ[NVIEW * 2:NVIEW * 3, :,  :]\n",
    "pred_mask = (maskLogit > 0).to(torch.bool)\n",
    "pred_mask_depth = pred_depth.clone()\n",
    "pred_mask_depth[maskLogit <= 0] = 0\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "axs[0].imshow(image[0, :3].cpu().permute(1, 2, 0))\n",
    "axs[1].imshow(pred_mask[viewid].cpu().numpy(), cmap='gray')\n",
    "axs[2].imshow((depth > 0)[viewid].to(torch.float).cpu().numpy(), cmap='gray')\n",
    "axs[3].imshow(pred_mask_depth[viewid].cpu().detach().numpy(), cmap='gray')\n",
    "axs[4].imshow(depth[viewid].cpu().detach().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all views\n",
    "fig, axs = plt.subplots(1, NVIEW, figsize=(NVIEW * 5, 5))\n",
    "for i in range(NVIEW):\n",
    "    axs[i].imshow(pred_mask[i].cpu().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Check all views\n",
    "fig, axs = plt.subplots(1, NVIEW, figsize=(NVIEW * 5, 5))\n",
    "for i in range(NVIEW):\n",
    "    axs[i].imshow((depth > 0)[i].to(torch.float).cpu().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
